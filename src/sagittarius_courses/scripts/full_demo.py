import dashscope
from dashscope import MultiModalConversation
import base64
import cv2
import numpy as np
import os
from cv2 import Mat
image: Mat
#!/usr/bin/env python
# -*- coding: utf-8 -*-
from sensor_msgs.msg import Image
import rospy, sys
import moveit_commander
from cv_bridge import CvBridge, CvBridgeError
fix_position = []
for i in range(8):
    fix_position.append([0,0])
fix_position[0][0] = [-0.40,-0.33,-0.645,0,0.38,1.57]
fix_position[0][1] = [-0.4,-0.27,-0.24,0,0.38,1.57]
fix_position[1][0] = [-0.30,-0.37,-0.645,0,0.38,1.57]
fix_position[1][1] = [-0.3,-0.27,-0.14,0,0.38,1.57]
fix_position[2][0] = [-0.20,-0.37,-0.645,0,0.38,1.57]
fix_position[2][1] = [-0.2,-0.27,-0.14,0,0.38,1.57]
fix_position[3][0] = [-0.10,-0.37,-0.645,0,0.38,1.57]
fix_position[3][1] = [-0.1,-0.27,-0.14,0,0.38,1.57]
fix_position[4][0] = [0,-0.32,-0.77,0,0.48,1.57]
fix_position[4][1] = [0,-0.4,-0.5,0,0.48,1.57]
fix_position[5][0] = [0.15,-0.32,-0.77,0,0.48,1.57]
fix_position[5][1] = [0.15,-0.4,-0.5,0,0.48,1.57]
fix_position[6][0] = [0.30,-0.37,-0.645,0,0.38,1.57]
fix_position[6][1] = [0.3,-0.27,-0.14,0,0.38,1.57]
fix_position[7][0] = [0.40,-0.33,-0.645,0,0.38,1.57]
fix_position[7][1] = [0.4,-0.27,-0.24,0,0.38,1.57]
class MoveItGripperDemo:
    def __init__(self,position):
        # åˆå§‹åŒ–move_groupçš„API
        global fix_position
        moveit_commander.roscpp_initialize(sys.argv)

        # åˆå§‹åŒ–ROSèŠ‚ç‚¹
        rospy.init_node('moveit_gripper_demo', anonymous=True)
        arm = moveit_commander.MoveGroupCommander("sagittarius_arm")
        # åˆå§‹åŒ–éœ€è¦ä½¿ç”¨move groupæ§åˆ¶çš„å¤¹çˆªçš„group
        gripper = moveit_commander.MoveGroupCommander("sagittarius_gripper")

        # è®¾ç½®å¤¹çˆªè¿åŠ¨çš„å…è®¸è¯¯å·®å€¼
        arm.allow_replanning(True)
        
        # è®¾ç½®ç›®æ ‡ä½ç½®æ‰€ä½¿ç”¨çš„å‚è€ƒåæ ‡ç³»
        arm.set_pose_reference_frame('world')
                
        # è®¾ç½®ä½ç½®(å•ä½ï¼šç±³)å’Œå§¿æ€ï¼ˆå•ä½ï¼šå¼§åº¦ï¼‰çš„å…è®¸è¯¯å·®
        arm.set_goal_position_tolerance(0.001)
        arm.set_goal_orientation_tolerance(0.001)
        
        # è®¾ç½®å…è®¸çš„æœ€å¤§é€Ÿåº¦å’ŒåŠ é€Ÿåº¦
        arm.set_max_acceleration_scaling_factor(0.5)
        arm.set_max_velocity_scaling_factor(0.5)
        gripper.set_goal_joint_tolerance(0.001)

        # è®¾ç½®å…è®¸çš„æœ€å¤§é€Ÿåº¦å’ŒåŠ é€Ÿåº¦
        gripper.set_max_acceleration_scaling_factor(0.5)
        gripper.set_max_velocity_scaling_factor(0.5)
        arm.set_named_target('sleep')
        arm.go()
        rospy.sleep(1)
        # æ§åˆ¶å¤¹çˆªé—­åˆ
        gripper.set_named_target('close')
        gripper.go()
        rospy.sleep(1)
        # joint_positions = [-0.00061454830783556, -0.2034660042252271, -0.149119908946021,
        #                    0.02245819323134223, -0.4085151620609834, 0.00039724354387525]
        joint_positions_1 = fix_position[position-1][0]
        arm.set_joint_value_target(joint_positions_1)
        arm.go()
        rospy.sleep(1)
        joint_positions_2 = fix_position[position-1][1]
        arm.set_joint_value_target(joint_positions_2)
        arm.go()
        rospy.sleep(1)
        #è®©æœºæ¢°è‡‚çš„æœ«ç«¯ç§»åŠ¨åˆ°æŒ‡å®šä½ç½®
        
        # æ§åˆ¶å¤¹çˆªæ‰“å¼€
        # gripper.set_named_target('open')
        # gripper.go()
        # rospy.sleep(2)

        # æ§åˆ¶å¤¹çˆªé—­åˆ
        # gripper.set_named_target('close')
        # gripper.go()
        # rospy.sleep(2)

        # # è®¾ç½®å¤¹çˆªçš„ç›®æ ‡ä½ç½®ï¼Œä½¿ç”¨ä¸¤ä¸ªå…³èŠ‚çš„ä½ç½®æ•°æ®è¿›è¡Œæè¿°ï¼ˆå•ä½ï¼šå¼§åº¦ï¼‰
        # joint_positions = [-0.022, -0.022]
        # gripper.set_joint_value_target(joint_positions)

        # # æ§åˆ¶å¤¹çˆªå®Œæˆè¿åŠ¨
        # gripper.go()
        # rospy.sleep(2)

        # # æ§åˆ¶å¤¹çˆªå…ˆå›åˆ°åˆå§‹åŒ–ä½ç½®
        # gripper.set_named_target('open')
        # gripper.go()
        rospy.sleep(1)
        arm.set_named_target('sleep')
        arm.go()
        rospy.sleep(5)
        # å…³é—­å¹¶é€€å‡ºmoveit
        # moveit_commander.roscpp_shutdown()
        # moveit_commander.os._exit(0)

def callback(data):
    global latest_image
    try:
        bridge = CvBridge()
        # å°†ROSå›¾åƒæ¶ˆæ¯è½¬æ¢ä¸ºOpenCVå›¾åƒ
        latest_image = bridge.imgmsg_to_cv2(data, "bgr8")
        cv2.imshow("Camera Image", latest_image)
        cv2.waitKey(1)
    except CvBridgeError as e:
        print(e)
# è®¾ç½® DashScope API Key
dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")  # æˆ–è€…å†™æˆ 'your_api_key_here'

class QwenMultiTurnChat:
    def __init__(self):
        self.history = []

    def add_user_message(self, prompt=None, image_base64=None):
        """æ·»åŠ ç”¨æˆ·æ¶ˆæ¯ï¼Œæ”¯æŒæ–‡æœ¬å’Œ Base64 å›¾ç‰‡"""
        content = []
        if image_base64:
            content.append({'image': f'data:image/png;base64,{image_base64}'})
        if prompt:
            content.append({'text': prompt})
        self.history.append({'role': 'user', 'content': content})

    def get_assistant_response(self):
        """è°ƒç”¨ Qwen2.5-VL è·å–å›å¤"""
        response = MultiModalConversation.call(
            model='qwen2.5-vl',
            input={'messages': self.history}
        )
        reply_text = response.output.text
        self.history.append({'role': 'assistant', 'content': [{'text': reply_text}]})
        return reply_text

    def chat_round(self, prompt, image=None):
        """
        æ‰§è¡Œä¸€è½®å¯¹è¯ï¼šç”¨æˆ·æé—® + æ¨¡å‹å›å¤
        :param prompt: ç”¨æˆ·çš„æ–‡æœ¬æç¤ºè¯
        :param image: OpenCV å›¾åƒå¯¹è±¡ (numpy.ndarray)
        :return: åŠ©æ‰‹çš„å›å¤
        """
        image_base64 = None
        if isinstance(image, np.ndarray):
            # å°† OpenCV å›¾åƒè½¬ä¸º Base64
            _, buffer = cv2.imencode('.jpg', image)  # å¯æ”¹ä¸º .png
            image_base64 = base64.b64encode(buffer).decode('utf-8')
        self.add_user_message(prompt=prompt, image_base64=image_base64)
        return self.get_assistant_response()
if __name__ == '__main__':
    chatbot = QwenMultiTurnChat()

    # ç¬¬ä¸€è½®å¯¹è¯ï¼šå‘é€æœ¬åœ°å›¾ç‰‡ + æè¿°å›¾ç‰‡å†…å®¹
     # æ›¿æ¢ä¸ºä½ æœ¬åœ°å›¾ç‰‡çš„è·¯å¾„
    prompt = "ä½ æ˜¯ä¸€ä¸ªéº»å°†é«˜æ‰‹ï¼Œæˆ‘éœ€è¦ä½ å¸®æˆ‘å†³ç­–æ‰“å“ªä¸€å¼ ï¼Œè®©æˆ‘ä»¬å…ˆè¯†åˆ«ä¸€ä¸‹æ¯ä¸€å¼ ç‰Œï¼Œä½ è¦è®°ä½å®ƒä»¬"
    response = chatbot.chat_round(prompt=prompt)
    print("ã€åŠ©æ‰‹ã€‘:", response)

    # ç¬¬äºŒè½®å¯¹è¯ï¼šç»§ç»­æé—®
    image_path="wan.jpg"
    prompt = "è¿™äº›ç‰Œä»å·¦åˆ°å³æ˜¯ä¸€ä¸‡åˆ°ä¹ä¸‡"
    image = cv2.imread(image_path)
    if image is None:
        print(f"æ— æ³•åŠ è½½å›¾åƒï¼š{image_path}")
    else:
        response = chatbot.chat_round(prompt=prompt, image=image)
        print("ã€åŠ©æ‰‹ã€‘:", response)

    # ç¬¬ä¸‰è½®å¯¹è¯ï¼šç»§ç»­æé—®
    image_path = "tong.jpg"
    prompt = "è¿™äº›ç‰Œä»å·¦åˆ°å³æ˜¯ä¸€ç­’åˆ°ä¹ç­’"
    if image is None:
        print(f"æ— æ³•åŠ è½½å›¾åƒï¼š{image_path}")
    else:
        response = chatbot.chat_round(prompt=prompt, image=image)
        print("ã€åŠ©æ‰‹ã€‘:", response)
    
    
    # ç¬¬ä¸‰è½®å¯¹è¯ï¼šç»§ç»­æé—®
    image_path = "tiao.jpg"
    prompt = "è¿™äº›ç‰Œä»å·¦åˆ°å³æ˜¯ä¸€æ¡åˆ°ä¹æ¡"
    if image is None:
        print(f"æ— æ³•åŠ è½½å›¾åƒï¼š{image_path}")
    else:
        response = chatbot.chat_round(prompt=prompt, image=image)
        print("ã€åŠ©æ‰‹ã€‘:", response)
    rospy.init_node('get_image_node', anonymous=True)
    sub = rospy.Subscriber("/usb_cam/image_raw", Image, callback)
    rospy.spin()  # ä¿æŒèŠ‚ç‚¹è¿è¡Œ
    while True:
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break
        elif key == ord('s'):
            print("ğŸ“¸ æ­£åœ¨å‘é€å½“å‰ç”»é¢...")
            prompt = "ç°åœ¨åœºä¸Šæœ‰å…«å¼ ç‰Œï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç»™å‡ºæƒ³æ‰“çš„ç‰Œï¼šæˆ‘æƒ³æ‰“â€œ â€ç‰Œ, æ˜¯ä»å·¦åˆ°å³ç¬¬â€œ â€å¼  "
            response = chatbot.chat_round(prompt=prompt, image=latest_image)
            if image is None:
                print(f"æ— æ³•åŠ è½½å›¾åƒï¼š{image_path}")
            else:
                response = chatbot.chat_round(prompt=prompt, image=image)
            num = response[-2]
            if num.isdigit():
                MoveItGripperDemo(num)
            else:
                print(f"æ— æ³•è¯†åˆ«å›¾åƒï¼š{image_path}")



